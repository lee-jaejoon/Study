# 8. Contraction Rates: General Theory
## 8.1 Introduction

### Definition 8.1 (Contraction rate)

*A sequence $\epsilon_n$ is a **posterior contraction rate** at the parameter $\theta_0$ with respect to the semimetric $d$ if $\Pi_n( \theta : d(\theta, \theta_0) \ge M_n \epsilon_n \vert X^{(n)} ) \to 0$ in $P_{\theta_0}^{(n)}$-probability, for every $M_n \to \infty$. If all experiments share the same probability space and the convergence to zero takes place almost surely $[P_{\theta_0}^{(\infty)}]$, then $\epsilon_n$ is said to be a **posterior contraction rate in the strong sense**.*

* 위 정의의 statement 중, "the" contraction rate가 아니라 "a" contraction rate임에 주목.

  * $\epsilon_n$이 contraction rate이라면, 그보다 느리게 0으로 가는 $\epsilon^\prime_n$은 역시 contraction rate이 된다.
    $$
    0 \le \Pi_n( \theta : d(\theta, \theta_0) \ge M_n \epsilon_n^\prime \vert X^{(n)}) \le \Pi_n( \theta : d(\theta, \theta_0) \ge M_n \epsilon_n \vert X^{(n)}) \to 0
    $$

  * 우리는 이 rate 중 가장 빠르게 0으로 감소하는 $\epsilon_n$에 관심이 있다.

    * 그러나 이는 존재하지 않을 수 있고 관련된 성질을 정립하기도 어려움.
    * 따라서 이 감소하는 rate의 upper bound를 구하고 이것이 "optimal" rate와 같거나 혹은 가까우면 만족함.

### Lemma 8.2

*If $\Theta \subset \mathbb R$ and $E(\theta \vert X^{(n)}) = \theta_0 + O_p(\epsilon_n) $ and $V(\theta \vert X^{(n)}) = O_p(\epsilon_n^2) $, with respect to the distribution generated by the true parameter $\theta_0$, then $\epsilon_n$ is a rate of contraction at $\theta_0$ with respect to the Euclidean measure.*

* 이는 Posterior mean, variance가 explicit expression으로 표현되는 경우 적용할 수 있는 정리이다.

  * Example 8.3, 8.4 참고.

* Chebyshev's inequality에 의해, for any $M_n \to \infty$,
  $$
  \Pi_n\left( \theta : \vert\theta - E(\theta \vert X^{(n)})\vert \ge M_n \epsilon_n \Big\vert X^{(n)}\right) \le \frac{V(\theta \vert X^{(n)}) }{(M_n \epsilon_n)^2} \stackrel{P}{\to} 0
  $$

* $\vert E(\theta \vert X^{(n)}) - \theta_0 \vert = O_p(\epsilon_n) $이므로,
  $$
  \Pi_n\left( \theta : \left\vert E(\theta \vert X^{(n)}) - \theta_0 \right\vert \ge M_n \epsilon_n \Big\vert X^{(n)}\right)  \stackrel{P}{\to} 0
  $$

* 따라서
  $$
  \begin{align*}
  &\Pi_n\left( \theta : \left\vert \theta - \theta_0 \right\vert \ge M_n \epsilon_n \Big\vert X^{(n)}\right) \\
  &\le \Pi_n\left( \theta : \left\vert \theta - E(\theta \vert X^{(n)}) \right\vert \ge M_n \epsilon_n \Big\vert X^{(n)}\right)+\Pi_n\left( \theta : \left\vert E(\theta \vert X^{(n)}) - \theta_0 \right\vert \ge M_n \epsilon_n \Big\vert X^{(n)}\right) \stackrel{P}{\to} 0 
  
  \end{align*}
  $$

### Example 8.3 (Bernoulli)

$$
\begin{align*}
\theta &\sim \text{Be}(a, b) \\
X_1, \cdots, X_n \vert \theta &\stackrel{\text{iid}}{\sim} \text{Bin}(1, \theta) \\
\implies \quad \theta \vert X_1, \cdots, X_n  &\stackrel{\text{iid}}{\sim} \text{Be}\left(a + n\bar{X}_n, b + n - n \bar{X}_n \right) \\
\end{align*}
$$

### Example 8.4 (Uniform)

### Example 8.5 (Dirichlet process)

### Example 8.6 (White noise model)

### 

### Theorem 8.7

*If the posterior contraction rate at $\theta_0$ is $\epsilon_n$, then the center $\hat{\theta}_n$ of the smallest ball that contains posterior mass at least $1/2$ satisfies $d(\hat{\theta}_n, \theta_0 ) = O_p(\epsilon_n)$ in $P^{(n)}_{\theta_0}$-probability (or almost surely if the posterior contraction rate is in the strong sense).*

* Contraction of the posterior distribution at a rate implies the existence of point estimators that converge at the same rate.

### Theorem 8.8





## 8.2 Independent Identically Distributed Observations

### Settings

* 이 section에서는 probability density $p$를 parameter로 둔다.

  * $\mathcal P$ : a class of probability densities relative to a given dominating measure $\nu$ on a sample space $(\mathfrak X,\mathscr X)$.
  * $p_0$ : true density.

* Test function $\phi_n$ : measurable mapping $\phi_n : \mathfrak X^n \to [0,1]$.

  * Null hypothesis를 기각할 확률로 이해하면 된다.
  * 이 test function에 대한 적분을 아래와 같이 나타낸다.

  $$
  P^n \phi_n = E_p \phi_n(X_1, \cdots, X_n)= \int \phi_n dP^n \\
  P^n_0 \phi_n = E_{p_0} \phi_n(X_1, \cdots, X_n)= \int \phi_n dP_0^n \\
  $$

* 여기서는 아래 조건이 만족하는 metric $d$ on parameter space $\mathcal P$에 대하여 posterior contraction rate을 도출하고자 한다.

  * For every $n \in \mathbb N$ and $\epsilon > 0$ and $p_1$ with $d(p_1, p_0) > \epsilon$, there exists a **test** $\phi_n$ with, for some universal constants $\xi, K > 0$, 
    $$
    P_0^n \phi_n \le e^{-K n \epsilon^2}, \quad \sup_{d(p,p_1) < \xi \epsilon} P^n(1-\phi_n) \le e^{-K n \epsilon^2}. \quad \quad \cdots\cdots(a)
    $$

* Probability density의 metric $d$에 대한 위 조건은 다음과 같이 이해할 수 있다.

  * Let $p_0 \in \mathcal P$ and 
    $$
    U = \Big\{ p \in \mathcal P : d(p, p_0) \le \epsilon \Big\}
    $$

  * 아래와 같은 검정을 수행한다고 해보자.
    $$
    H_0 : p = p_0 \enspace \text{ vs }\enspace H_1 : p \in U^c =\Big\{ p \in \mathcal P : d(p, p_0) > \epsilon \Big\}
    $$

  * Test function $\phi_n(X_1, \cdots, X_n)$은 $[0,1]$의 값을 갖는 measurable mapping이다.

  * $\phi_n(X_1, \cdots, X_n)$는 관측된 sample을 바탕으로 검정을 수행하는 randomized decision rule로 생각할 수 있다.

    * $X_1, \cdots , X_n$이 관측되었을 때 아래 확률에 따라 random하게 action을 수행한다고 하자.

      * Accept $H_0$ with probability $1-\phi_n(X_1, \cdots, X_n)$.
      * Reject $H_0$ with probability $\phi_n(X_1, \cdots, X_n)$.

    * 0-1 loss function을 가정한 검정에서는 loss와 risk가 아래와 같다.
      $$
      \begin{align*}
      \ell(p, \delta^\ast(X, \cdot)) &= \int_{\mathcal A} \ell(p, a) \delta^\ast(X, da) \\
      &= \ell(p, \text{accept }H_0) (1-\phi_n) + \ell(p, \text{accept }H_1)\phi_n \\
      &= 
      \begin{cases}
      \phi_n & \text{if }p=p_0\\
      1-\phi_n & \text{if }p \in U^c\\
      \end{cases}
      \end{align*}
      $$

      $$
      \begin{align*}
      R(p, \delta^\ast) &= E_{X\sim p}\ell(p, \delta^\ast(X, \cdot)) \\
      &= \begin{cases}
      E_{X\sim p_0}\phi_n & \text{if }p=p_0\\
      1-E_{X\sim p}\phi_n & \text{if }p \in U^c\\
      \end{cases}
      \end{align*}
      $$

    * 이는 각각 type 1 error probability, type 2 error probability가 된다.

  * 만약 $n$이 증가함에 따라 이 type 1 error probability, type 2 error probability가 $0$으로 수렴한다면, sample size $n$에 대한 test function의 sequence $\{ \phi_n(X_1, \cdots, X_n) \}_{n=1}^\infty$는 **uniformly consistent**하다고 한다.

    * 두 error probability가 $0$으로 지수적인 속도로 수렴하는 경우, **uniformly exponentially consistent**하다고 한다.

  * 이제 probability density의 metric $d$에 대해 위에서 제시된 조건의 의미를 이해할 수 있다.

    * For every $n \in \mathbb N$ and $\epsilon > 0$ and $p_1$ with $d(p_1, p_0) > \epsilon$, there exists a **test** $\phi_n$ with, for some universal constants $\xi, K > 0$, 

    $$
    P_0^n \phi_n \le e^{-K n \epsilon^2}, \quad \sup_{d(p,p_1) < \xi \epsilon} P^n(1-\phi_n) \le e^{-K n \epsilon^2}.
    $$

* 또한 아래에 소개할 정리는 prior mass concentration rate와 model의 entropy에 대하여 posterior contraction rate을 기술한다.

  * 그 중 concentration rate을 나타내기 위해, Kullback-Leibler divergence $K(p_0; p)$와 $k^{\text{th}}$ Kullback-Leibler variation $V_{k,0}(p_0; p)$로 아래와 같이 probability density의 공간 내의 neighborhood를 정의한다.

  $$
  \begin{align*}
  K(p_0; p) &= P_0 \log(\frac{p_0}{p})\\
  V_{k,0}(p_0; p) &= P_0 \left\vert \log(\frac{p_0}{p}) - K(p_0; p) \right\vert^k = P_0 \left\vert \log(\frac{p_0}{p}) - P_0 \log(\frac{p_0}{p}) \right\vert^k\\
  B_0(p_0, \epsilon) &=\Big\{ p \in \mathcal P : K(p_0; p) < \epsilon^2 \Big\} \\
  B_k(p_0, \epsilon) &=\Big\{ p \in \mathcal P : K(p_0; p) < \epsilon^2, V_{k,0}(p_0; p) < \epsilon^k \Big\}
  \end{align*}
  $$




### Packing Number, Covering Number

#### Definitions

* *A subset $S$ of a semimetric space $(T,d )$ is said to be **$\epsilon$-dispersed** if $d(s, s^\prime) \ge \epsilon$ for all $s, s^\prime \in S$ with $s \neq s^\prime$.* 
* *The maximum cardinality of an $\epsilon$-dispersed subset of $T$ is known as the **$\epsilon$-packing number** of $T$ and is denoted by $D(\epsilon, T, d)$.*
  * 모든 원소가 서로에게서 $\epsilon$ 이상 떨어져 있는 점들을 최대 몇 개나 잡을 수 있는가?
  * $\epsilon$-dispersed set은 모든 원소가 서로에게서 $\epsilon$ 이상 떨어져 있는 set.
* *A set $S$ is called an **$\epsilon$-net** for $T$ if for every $t \in T$, there exists $s \in S$ such that $d(s,t) < \epsilon$, or equivalently $T$ is covered by the collection of balls of radius $\epsilon$ around the points in $S$.*
* *The minimal cardinality of an $\epsilon$-net is known as the **$\epsilon$-covering number** of $T$ and is denoted by $N(\epsilon, T, d )$.*
  * $T$를 빠짐없이 덮으려면 radius $\epsilon$의 ball들을 몇개나 사용해야 하는가?
  * $\epsilon$-net for $T$는 각 원소를 center로 하는 radius $\epsilon$ ball을 그렸을 때 $T$를 덮을 수 있는 set.
* *The logarithm of the packing number is called the **entropy**.*
* *The logarithm of the covering number is called the **metric entropy**.*

#### Remarks

* 위에서 정의한 packing, covering number에 대해서 아래와 같은 bound가 만족한다.
  $$
  N(\epsilon, T, d) \stackrel{(a)}{\le} D(\epsilon, T, d) \stackrel{(b)}{\le} N(\epsilon/2, T, d) \enspace \text{ for every }\epsilon > 0
  $$

* 이는 아래와 같이 확인할 수 있다.

  * $(a)$

    * $S = \{ \theta_1, \cdots, \theta_M \}$이 maximal packing of $T$라고 하자.

      * $\vert S \vert = D(\epsilon, T, d)$

    * Then $ \forall \theta \in T \backslash S$, $\exists i$ such that $ d( \theta , \theta_i ) \le \epsilon$.

      * 모든 $\theta_i$에 대해 $\epsilon$보다 멀리 떨어져 있는 점 $\theta$가 있다면?
      * 그 $S$보다 더 큰 packing $ S \cup \{ \theta \}$을 만들 수 있으므로, $S$가 maximal packing이라는 것에 모순이 되므로.

    * 따라서 $S$는 $\epsilon$-covering of $T$가 된다.

    * $N(\epsilon, T, d)$는 minimal $\epsilon$-covering의 cardinality이므로
      $$
      N(\epsilon, T, d) \le \vert S \vert = D(\epsilon, T, d)
      $$

  * $(b)$

    * Proof by contradiction을 위해, 아래와 같은 상황을 가정하자.

      * $\epsilon$-packing $\{ \theta_1, \cdots, \theta_M\}$
      * $\epsilon/2$-covering $\{ x_1, \cdots, x_N \}$
      * $M > N$

    * $M > N$이므로, 같은 $\epsilon/2$-ball $B_{\epsilon/2}(x_k)$에 속하는 $\theta_i , \theta_j $가 존재한다.

    * 따라서 $d(\theta_i, \theta_j) \le d(\theta_i, \theta) + d(\theta_j, \theta) < \epsilon$.

      * 이는 $\{ \theta_1, \cdots, \theta_M\}$이 $\epsilon$-packing이라는 가정에 모순이 된다.

    * 그러므로 
      $$
      M = D(\epsilon, T, d) \le N(\epsilon/2, T, d) = N
      $$

### Testing and Metric Entropy







### Theorem 8.9 (Basic contraction rate)

*Given a distance $d$ for which the condition (a) is satisfied, suppose that there exist partitions $\mathcal P = {\mathcal P}_{n,1} \cup {\mathcal P}_{n,2}$ and a constant $C > 0$, such that, for constants ${\bar \epsilon}_n \le \epsilon_n$ with $n{\bar \epsilon}_n^2 \to \infty$,*
$$
\begin{align*}
\text{(i)} \quad &\Pi_n(B_2(p_0, {\bar \epsilon}_n)) \ge e^{-C n {\bar \epsilon}_n^2}. \\
\text{(ii)} \quad &\log N(\xi \epsilon_n, \mathcal P_{n,1}, d) \le n \epsilon_n^2. \\
\text{(iii)} \quad &\Pi_n(\mathcal P_{n,2}) \le e^{-(C+4) n {\bar \epsilon}_n^2}.
\end{align*}
$$
*Then the posterior rate of contraction at $p_0$ is $\epsilon_n$. If $\epsilon \succsim n^{-\alpha} $ for some $\alpha \in (0,1/2)$ and $(\text{i})$ holds with $B_2(p_0, {\bar \epsilon}_n)$ replaced by $B_k(p_0, {\bar \epsilon}_n)$ for some $k$ such that $k(1-2\alpha) > 2$, then the contraction rate is also in the almost sure sense $[P_0^\infty]$.*

* 























